{
  "total_questions": 11,
  "qa_pairs": [
    {
      "question": "What is gradient descent? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]",
      "answer": "(https://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n\nGradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n\nGradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm."
    },
    {
      "question": "Explain over- and under-fitting and how to combat them? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]",
      "answer": "(https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)\n\nML/DL models essentially learn a relationship between its given inputs(called training features) and objective outputs(called labels). Regardless of the quality of the learned relation(function), its performance on a test set(a collection of data different from the training input) is subject to investigation.\n\nMost ML/DL models have trainable parameters which will be learned to build that input-output relationship. Based on the number of parameters each model has, they can be sorted into more flexible(more parameters) to less flexible(less parameters).\n\nThe problem of Underfitting arises when the flexibility of a model(its number of parameters) is not adequate to capture the underlying pattern in a training dataset. Overfitting, on the other hand, arises when the model is too flexible to the underlying pattern. In the later case it is said that the model has “memorized” the training data.\n\nAn example of underfitting is estimating a second order polynomial(quadratic function) with a first order polynomial(a simple line). Similarly, estimating a line with a 10th order polynomial would be an example of overfitting."
    },
    {
      "question": "Explain Principal Component Analysis (PCA)? [[src](http://houseofbots.com/news-detail/2849-4-data-science-and-machine-learning-interview-questions)]",
      "answer": "(https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning to reduce the number of features in a dataset while retaining as much information as possible. It works by identifying the directions (principal components) in which the data varies the most, and projecting the data onto a lower-dimensional subspace along these directions."
    },
    {
      "question": "Implement a sparse matrix class in C++. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]",
      "answer": "(https://www.geeksforgeeks.org/sparse-matrix-representation/)"
    },
    {
      "question": "Create a function to compute an [integral image](https://en.wikipedia.org/wiki/Summed-area_table), and create another function to get area sums from the integral image.[[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]",
      "answer": "(https://www.geeksforgeeks.org/submatrix-sum-queries/)"
    },
    {
      "question": "How does [CBIR](https://www.robots.ox.ac.uk/~vgg/publications/2013/arandjelovic13/arandjelovic13.pdf) work? [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]",
      "answer": "(https://en.wikipedia.org/wiki/Content-based_image_retrieval)\nContent-based image retrieval is the concept of using images to gather metadata on their content. Compared to the current image retrieval approach based on the keywords associated to the images, this technique generates its metadata from computer vision techniques to extract the relevant informations that will be used during the querying step. Many approach are possible from feature detection to retrieve keywords to the usage of CNN to extract dense features that will be associated to a known distribution of keywords. \n\nWith this last approach, we care less about what is shown on the image but more about the similarity between the metadata generated by a known image and a list of known label and or tags projected into this metadata space."
    },
    {
      "question": "Reverse a linked list in place. [[src](https://www.reddit.com/r/computervision/comments/7gku4z/technical_interview_questions_in_cv/)]",
      "answer": "(https://www.geeksforgeeks.org/reverse-a-linked-list/)"
    },
    {
      "question": "Explain how a ROC curve works.",
      "answer": "(https://www.springboard.com/blog/machine-learning-interview-questions/)\nThe ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives)."
    },
    {
      "question": "What’s the difference between Type I and Type II error?",
      "answer": "(https://www.springboard.com/blog/machine-learning-interview-questions/)\nType I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it hasn’t, while Type II error means that you claim nothing is happening when in fact something is.\nA clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby."
    },
    {
      "question": "What’s the difference between a generative and discriminative model?",
      "answer": "(https://www.springboard.com/blog/machine-learning-interview-questions/)\nA generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks."
    },
    {
      "question": "What is the difference between Bayesian vs frequentist statistics?",
      "answer": "(https://www.kdnuggets.com/2022/10/nlp-interview-questions.html)\nFrequentist statistics is a framework that focuses on estimating population parameters using sample statistics, and providing point estimates and confidence intervals.\n\nBayesian statistics, on the other hand, is a framework that uses prior knowledge and information to update beliefs about a parameter or hypothesis, and provides probability distributions for parameters.\n\nThe main difference is that Bayesian statistics incorporates prior knowledge and beliefs into the analysis, while frequentist statistics doesn't.\n\n### 66) What is the basic difference between LSTM and Transformers? \nLSTMs (Long Short Term Memory) models consist of RNN cells designed to store and manipulate information across time steps more efficiently. In contrast, Transformer models contain a stack of encoder and decoder layers, each consisting of self attention and feed-forward neural network components. \n\n### 66) What are RCNNs? \nRecurrent Convolutional model is a model that is specially designed to make predictions using a sequence of images (more commonly also know as video). These models are used in object detection tasks in computer vision. The RCNN approach combines both region proposal techniques and convolutional neural networks (CNNs) to identify and locate objects within an image.\n\n\n## Contributions\nContributions are most welcomed.\n 1. Fork the repository.\n 2. Commit your *questions* or *answers*.\n 3. Open **pull request**."
    }
  ]
}